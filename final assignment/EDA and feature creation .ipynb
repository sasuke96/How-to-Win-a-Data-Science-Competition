{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load all the required libraries and load raw data files into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import catboost\n",
    "from catboost import Pool\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import product\n",
    "import time\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories=pd.read_csv(\"item_categories.csv\")\n",
    "item=pd.read_csv(\"items.csv.filepart\")\n",
    "shops=pd.read_csv(\"shops.csv\")\n",
    "test_data=pd.read_csv(\"test.csv.gz.filepart\", compression='gzip')\n",
    "sample_submission=pd.read_csv(\"sample_submission.csv.gz.filepart\", compression='gzip')\n",
    "train_data=pd.read_csv(\"sales_train.csv.gz.filepart\",compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_context(\"talk\", font_scale=1.0)\n",
    "sales_month = pd.DataFrame(train_data.groupby(['date_block_num']).sum().item_cnt_day).reset_index()\n",
    "sales_month.columns = ['date_block_num', 'sum_items_sold']\n",
    "sns.barplot(x ='date_block_num', y='sum_items_sold', \n",
    "            data=sales_month.reset_index());\n",
    "sns.set(rc={'figure.figsize':(14,10)})\n",
    "plt.plot(sales_month.sum_items_sold)\n",
    "plt.title('Distribution of the sum of sales per month')\n",
    "del sales_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comb_shop_item = pd.DataFrame(train_data[['date_block_num', 'shop_id', \n",
    "                                     'item_id']].drop_duplicates().groupby('date_block_num').size()).reset_index()\n",
    "comb_shop_item.columns = ['date_block_num', 'item-shop_comb']\n",
    "sns.barplot(x ='date_block_num', y='item-shop_comb', data=comb_shop_item)\n",
    "sns.set(rc={'figure.figsize':(14,10)})\n",
    "plt.plot(comb_shop_item['item-shop_comb']);\n",
    "plt.title('Number of combinations shop-item with sales per month')\n",
    "del comb_shop_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_context(\"talk\", font_scale=1)\n",
    "sales_month_shop_id = pd.DataFrame(train_data.groupby(['shop_id']).sum().item_cnt_day).reset_index()\n",
    "sales_month_shop_id.columns = ['shop_id', 'sum_sales']\n",
    "sns.barplot(x ='shop_id', y='sum_sales', data=sales_month_shop_id, palette='Paired')\n",
    "plt.title('Distribution of sales per shop');\n",
    "del sales_month_shop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fce3ef01be0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set_context(\"talk\", font_scale=1.4)\n",
    "sales_item_id = pd.DataFrame(train_data.groupby(['item_id']).sum().item_cnt_day)\n",
    "plt.xlabel('item id')\n",
    "plt.ylabel('sales')\n",
    "plt.plot(sales_item_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one item with an anomaly number of sales. The item id is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/python35/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20949"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_item = sales_item_id.item_cnt_day.argmax()\n",
    "anomaly_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fig, ax = plt.subplots()\n",
    "sns.set_context(\"talk\", font_scale=0.8)\n",
    "sales_item_cat = train_data.merge(item, how='left', on='item_id').groupby('item_category_id').item_cnt_day.sum()\n",
    "sns.barplot(x ='item_category_id', y='item_cnt_day',\n",
    "            data=sales_item_cat.reset_index(), \n",
    "            palette='Paired'\n",
    "           );\n",
    "del sales_item_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers in item count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce32b3f160>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(-100, 3000)\n",
    "sns.boxplot(x=train_data.item_cnt_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier in Item price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce37819cf8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(train_data.item_price.min(), train_data.item_price.max()*1.1)\n",
    "sns.boxplot(x=train_data.item_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data.item_price<100000]\n",
    "train_data = train_data[train_data.item_cnt_day<1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is one item with price <0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484683</th>\n",
       "      <td>15.05.2013</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2973</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n",
       "484683  15.05.2013               4       32     2973        -1.0           1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data.item_price<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is one item with price below zero. Fill it with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = train_data[(train_data.shop_id==32)&(train_data.item_id==2973)&(train_data.date_block_num==4)&(train_data.item_price>0)].item_price.median()\n",
    "train_data.loc[train_data.item_price<0, 'item_price'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several shops are duplicates of each other (according to its name). Fix train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "train_data.loc[train_data.shop_id == 0, 'shop_id'] = 57\n",
    "test_data.loc[test_data.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "train_data.loc[train_data.shop_id == 1, 'shop_id'] = 58\n",
    "test_data.loc[test_data.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "train_data.loc[train_data.shop_id == 10, 'shop_id'] = 11\n",
    "test_data.loc[test_data.shop_id == 10, 'shop_id'] = 11\n",
    "#РостовНаДону ТРК \"Мегацентр Горизонт\"\n",
    "train_data.loc[train_data.shop_id == 39, 'shop_id'] = 40\n",
    "test_data.loc[test_data.shop_id == 39, 'shop_id'] = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shops/Cats/Items preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Each shop_name starts with the city name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each category contains type and subtype in its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "shops = shops[['shop_id','city_code']]\n",
    "\n",
    "\n",
    "item_categories['split'] = item_categories['item_category_name'].str.split('-')\n",
    "item_categories['type'] = item_categories['split'].map(lambda x: x[0].strip())\n",
    "item_categories['type_code'] = LabelEncoder().fit_transform(item_categories['type'])\n",
    "item_categories['subtype'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "item_categories['subtype_code'] = LabelEncoder().fit_transform(item_categories['subtype'])\n",
    "item_categories = item_categories[['item_category_id','type_code', 'subtype_code']]\n",
    "item.drop(['item_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. Hence, for the most of the items in the test set target value should be zero. In the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and extend it with zero sales for each unique pair within the month. This way train data will be similar to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 5100, 214200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(test_data.item_id) - set(test_data.item_id).intersection(set(train_data.item_id)))), len(list(set(test_data.item_id))), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "for i in range(34):\n",
    "    sales = train_data[train_data.date_block_num==i]\n",
    "    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n",
    "\n",
    "matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n",
    "matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n",
    "matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n",
    "matrix['item_id'] = matrix['item_id'].astype(np.int16)\n",
    "matrix.sort_values(cols,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114910</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117150</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120623</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118316</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114602</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  shop_id  item_id\n",
       "114910               0        2       19\n",
       "117150               0        2       27\n",
       "120623               0        2       28\n",
       "118316               0        2       29\n",
       "114602               0        2       32"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate train set by shop/item pairs to calculate target aggreagates, then clip(0,20) target value. This way train target will be similar to the test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['revenue'] = train_data['item_price'] *  train_data['item_cnt_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = train_data.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n",
    "group.columns = ['item_cnt_month']\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(matrix, group, on=cols, how='left')\n",
    "matrix['item_cnt_month'] = (matrix['item_cnt_month']\n",
    "                                .fillna(0)\n",
    "                                .clip(0,20) # NB clip target here\n",
    "                                .astype(np.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use time tricks append test pairs to the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['date_block_num'] = 34\n",
    "test_data['date_block_num'] = test_data['date_block_num'].astype(np.int8)\n",
    "test_data['shop_id'] = test_data['shop_id'].astype(np.int8)\n",
    "test_data['item_id'] = test_data['item_id'].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num  shop_id  item_id  item_cnt_month   ID\n",
       "0               0        2       19             0.0  0.0\n",
       "1               0        2       27             1.0  0.0\n",
       "2               0        2       28             0.0  0.0\n",
       "3               0        2       29             0.0  0.0\n",
       "4               0        2       32             0.0  0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = pd.concat([matrix, test_data], ignore_index=True, sort=False, keys=cols)\n",
    "matrix.fillna(0, inplace=True)#34 month\n",
    "matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shops/Items/Cats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\n",
    "matrix = pd.merge(matrix, item, on=['item_id'], how='left')\n",
    "matrix = pd.merge(matrix, item_categories, on=['item_category_id'], how='left')\n",
    "\n",
    "matrix['city_code'] = matrix['city_code'].astype(np.int8)\n",
    "matrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n",
    "matrix['type_code'] = matrix['type_code'].astype(np.int8)\n",
    "matrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean encoded Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\n",
    "matrix['date_avg_\"item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\n",
    "matrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\n",
    "matrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_shop_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\n",
    "matrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_cat_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\n",
    "matrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\n",
    "matrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_cat_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\n",
    "matrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\n",
    "matrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_type_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\n",
    "matrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\n",
    "matrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_subtype_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\n",
    "matrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\n",
    "matrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_subtype_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\n",
    "matrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\n",
    "matrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_city_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\n",
    "matrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\n",
    "matrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_city_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\n",
    "matrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\n",
    "matrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_type_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\n",
    "matrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\n",
    "matrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_subtype_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\n",
    "matrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\n",
    "matrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price trend for the last six months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = train_data.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['item_id'], how='left')\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "group = train_data.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1,2,3,4,5,6]\n",
    "matrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = \\\n",
    "        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if row['delta_price_lag_'+str(i)]:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "\n",
    "matrix['delta_price_lag'].fillna(0, inplace=True)\n",
    "features_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    features_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n",
    "    features_to_drop += ['delta_price_lag_'+str(i)]\n",
    "\n",
    "matrix.drop(features_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last month shop revenue trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.88282752037048"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = train_data.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\n",
    "group.columns = ['date_shop_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n",
    "\n",
    "group = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\n",
    "group.columns = ['shop_avg_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['shop_id'], how='left')\n",
    "matrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n",
    "\n",
    "matrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n",
    "matrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n",
    "\n",
    "matrix = lag_feature(matrix, [1], 'delta_revenue')\n",
    "\n",
    "matrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix['month'] = matrix['date_block_num'] % 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of days in a month. There are no leap years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "matrix['days'] = matrix['month'].map(days).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Months since the last sale for each shop/item pair and for item only. I use programing approach.\n",
    "\n",
    "Create HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1831.6826212406158"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_shop_last_sale'] = -1\n",
    "matrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = str(row.item_id)+' '+str(row.shop_id)\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n",
    "        cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132.9767870903015"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_last_sale'] = -1\n",
    "matrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = row.item_id\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        if row.date_block_num>last_date_block_num:\n",
    "            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n",
    "            cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Months since the first sale for each shop/item pair and for item only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.503323793411255"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final preparations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3201770782470703"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix = matrix[matrix.date_block_num > 11]\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing lags brings a lot of nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.69538950920105"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "def fill_na(df):\n",
    "    for col in df.columns:\n",
    "        if ('_lag_' in col) & (df[col].isnull().any()):\n",
    "            if ('item_cnt' in col):\n",
    "                df[col].fillna(0, inplace=True)         \n",
    "    return df\n",
    "\n",
    "matrix = fill_na(matrix)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_block_num', 'shop_id', 'item_id', 'item_cnt_month', 'ID',\n",
       "       'city_code', 'item_category_id', 'type_code', 'subtype_code',\n",
       "       'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
       "       'item_cnt_month_lag_6', 'item_cnt_month_lag_12', 'date_avg_\"item_cnt',\n",
       "       'date_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_1',\n",
       "       'date_item_avg_item_cnt_lag_2', 'date_item_avg_item_cnt_lag_3',\n",
       "       'date_item_avg_item_cnt_lag_6', 'date_item_avg_item_cnt_lag_12',\n",
       "       'date_shop_avg_item_cnt_lag_1', 'date_shop_avg_item_cnt_lag_2',\n",
       "       'date_shop_avg_item_cnt_lag_3', 'date_shop_avg_item_cnt_lag_6',\n",
       "       'date_shop_avg_item_cnt_lag_12', 'date_cat_avg_item_cnt_lag_1',\n",
       "       'date_shop_cat_avg_item_cnt_lag_1', 'date_shop_type_avg_item_cnt_lag_1',\n",
       "       'date_city_avg_item_cnt_lag_1', 'date_item_city_avg_item_cnt_lag_1',\n",
       "       'date_type_avg_item_cnt_lag_1', 'date_subtype_avg_item_cnt_lag_1',\n",
       "       'delta_price_lag', 'date_shop_subtype_avg_item_cnt_lag_1',\n",
       "       'delta_revenue_lag_1', 'month', 'days', 'item_shop_last_sale',\n",
       "       'item_last_sale', 'item_shop_first_sale', 'item_first_sale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6567567 entries, 4488710 to 11056276\n",
      "Data columns (total 42 columns):\n",
      "date_block_num                          int8\n",
      "shop_id                                 int8\n",
      "item_id                                 int16\n",
      "item_cnt_month                          float16\n",
      "ID                                      float64\n",
      "city_code                               int8\n",
      "item_category_id                        int8\n",
      "type_code                               int8\n",
      "subtype_code                            int8\n",
      "item_cnt_month_lag_1                    float16\n",
      "item_cnt_month_lag_2                    float16\n",
      "item_cnt_month_lag_3                    float16\n",
      "item_cnt_month_lag_6                    float16\n",
      "item_cnt_month_lag_12                   float16\n",
      "date_avg_\"item_cnt                      float16\n",
      "date_avg_item_cnt_lag_1                 float16\n",
      "date_item_avg_item_cnt_lag_1            float16\n",
      "date_item_avg_item_cnt_lag_2            float16\n",
      "date_item_avg_item_cnt_lag_3            float16\n",
      "date_item_avg_item_cnt_lag_6            float16\n",
      "date_item_avg_item_cnt_lag_12           float16\n",
      "date_shop_avg_item_cnt_lag_1            float16\n",
      "date_shop_avg_item_cnt_lag_2            float16\n",
      "date_shop_avg_item_cnt_lag_3            float16\n",
      "date_shop_avg_item_cnt_lag_6            float16\n",
      "date_shop_avg_item_cnt_lag_12           float16\n",
      "date_cat_avg_item_cnt_lag_1             float16\n",
      "date_shop_cat_avg_item_cnt_lag_1        float16\n",
      "date_shop_type_avg_item_cnt_lag_1       float16\n",
      "date_city_avg_item_cnt_lag_1            float16\n",
      "date_item_city_avg_item_cnt_lag_1       float16\n",
      "date_type_avg_item_cnt_lag_1            float16\n",
      "date_subtype_avg_item_cnt_lag_1         float16\n",
      "delta_price_lag                         float64\n",
      "date_shop_subtype_avg_item_cnt_lag_1    float16\n",
      "delta_revenue_lag_1                     float16\n",
      "month                                   int8\n",
      "days                                    int8\n",
      "item_shop_last_sale                     int8\n",
      "item_last_sale                          int8\n",
      "item_shop_first_sale                    int8\n",
      "item_first_sale                         int8\n",
      "dtypes: float16(27), float64(2), int16(1), int8(12)\n",
      "memory usage: 576.2 MB\n"
     ]
    }
   ],
   "source": [
    "matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_pickle('final_data.pkl')\n",
    "del matrix\n",
    "del cache\n",
    "del group\n",
    "del item\n",
    "del shops\n",
    "del item_categories\n",
    "del train_data\n",
    "#leave test for submission\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('final_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/python35/lib/python3.5/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/python/python35/lib/python3.5/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[06:39:54] WARNING: /workspace/src/learner.cc:686: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\tvalidation_0-rmse:1.13106\tvalidation_1-rmse:1.11888\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-rmse:1.08898\tvalidation_1-rmse:1.0823\n",
      "[2]\tvalidation_0-rmse:1.05926\tvalidation_1-rmse:1.05311\n",
      "[3]\tvalidation_0-rmse:1.01572\tvalidation_1-rmse:1.02692\n",
      "[4]\tvalidation_0-rmse:0.986575\tvalidation_1-rmse:1.00493\n",
      "[5]\tvalidation_0-rmse:0.965355\tvalidation_1-rmse:0.987329\n",
      "[6]\tvalidation_0-rmse:0.944534\tvalidation_1-rmse:0.973354\n",
      "[7]\tvalidation_0-rmse:0.927225\tvalidation_1-rmse:0.961875\n",
      "[8]\tvalidation_0-rmse:0.913474\tvalidation_1-rmse:0.953596\n",
      "[9]\tvalidation_0-rmse:0.901818\tvalidation_1-rmse:0.944881\n",
      "[10]\tvalidation_0-rmse:0.891735\tvalidation_1-rmse:0.938588\n",
      "[11]\tvalidation_0-rmse:0.883386\tvalidation_1-rmse:0.933596\n",
      "[12]\tvalidation_0-rmse:0.876518\tvalidation_1-rmse:0.929019\n",
      "[13]\tvalidation_0-rmse:0.869727\tvalidation_1-rmse:0.924446\n",
      "[14]\tvalidation_0-rmse:0.864385\tvalidation_1-rmse:0.921156\n",
      "[15]\tvalidation_0-rmse:0.860506\tvalidation_1-rmse:0.9195\n",
      "[16]\tvalidation_0-rmse:0.85681\tvalidation_1-rmse:0.918204\n",
      "[17]\tvalidation_0-rmse:0.853149\tvalidation_1-rmse:0.91597\n",
      "[18]\tvalidation_0-rmse:0.849898\tvalidation_1-rmse:0.914187\n",
      "[19]\tvalidation_0-rmse:0.846987\tvalidation_1-rmse:0.912378\n",
      "[20]\tvalidation_0-rmse:0.844328\tvalidation_1-rmse:0.911451\n",
      "[21]\tvalidation_0-rmse:0.841954\tvalidation_1-rmse:0.910261\n",
      "[22]\tvalidation_0-rmse:0.839947\tvalidation_1-rmse:0.909248\n",
      "[23]\tvalidation_0-rmse:0.837908\tvalidation_1-rmse:0.908666\n",
      "[24]\tvalidation_0-rmse:0.835953\tvalidation_1-rmse:0.908509\n",
      "[25]\tvalidation_0-rmse:0.834283\tvalidation_1-rmse:0.907557\n",
      "[26]\tvalidation_0-rmse:0.832758\tvalidation_1-rmse:0.906692\n",
      "[27]\tvalidation_0-rmse:0.831449\tvalidation_1-rmse:0.906822\n",
      "[28]\tvalidation_0-rmse:0.830033\tvalidation_1-rmse:0.906147\n",
      "[29]\tvalidation_0-rmse:0.828767\tvalidation_1-rmse:0.905697\n",
      "[30]\tvalidation_0-rmse:0.827468\tvalidation_1-rmse:0.90539\n",
      "[31]\tvalidation_0-rmse:0.825982\tvalidation_1-rmse:0.905503\n",
      "[32]\tvalidation_0-rmse:0.825164\tvalidation_1-rmse:0.905562\n",
      "[33]\tvalidation_0-rmse:0.824121\tvalidation_1-rmse:0.90568\n",
      "[34]\tvalidation_0-rmse:0.823209\tvalidation_1-rmse:0.905588\n",
      "[35]\tvalidation_0-rmse:0.82241\tvalidation_1-rmse:0.90527\n",
      "[36]\tvalidation_0-rmse:0.821531\tvalidation_1-rmse:0.905255\n",
      "[37]\tvalidation_0-rmse:0.820791\tvalidation_1-rmse:0.905137\n",
      "[38]\tvalidation_0-rmse:0.82016\tvalidation_1-rmse:0.904937\n",
      "[39]\tvalidation_0-rmse:0.819397\tvalidation_1-rmse:0.904716\n",
      "[40]\tvalidation_0-rmse:0.818709\tvalidation_1-rmse:0.905259\n",
      "[41]\tvalidation_0-rmse:0.81816\tvalidation_1-rmse:0.904929\n",
      "[42]\tvalidation_0-rmse:0.817723\tvalidation_1-rmse:0.904612\n",
      "[43]\tvalidation_0-rmse:0.817307\tvalidation_1-rmse:0.904546\n",
      "[44]\tvalidation_0-rmse:0.816672\tvalidation_1-rmse:0.904435\n",
      "[45]\tvalidation_0-rmse:0.81625\tvalidation_1-rmse:0.904163\n",
      "[46]\tvalidation_0-rmse:0.815846\tvalidation_1-rmse:0.903959\n",
      "[47]\tvalidation_0-rmse:0.815356\tvalidation_1-rmse:0.903852\n",
      "[48]\tvalidation_0-rmse:0.815077\tvalidation_1-rmse:0.903752\n",
      "[49]\tvalidation_0-rmse:0.814547\tvalidation_1-rmse:0.903513\n",
      "[50]\tvalidation_0-rmse:0.814185\tvalidation_1-rmse:0.903264\n",
      "[51]\tvalidation_0-rmse:0.813581\tvalidation_1-rmse:0.903606\n",
      "[52]\tvalidation_0-rmse:0.813042\tvalidation_1-rmse:0.903362\n",
      "[53]\tvalidation_0-rmse:0.812623\tvalidation_1-rmse:0.903324\n",
      "[54]\tvalidation_0-rmse:0.812312\tvalidation_1-rmse:0.903112\n",
      "[55]\tvalidation_0-rmse:0.811959\tvalidation_1-rmse:0.90293\n",
      "[56]\tvalidation_0-rmse:0.811296\tvalidation_1-rmse:0.902637\n",
      "[57]\tvalidation_0-rmse:0.810778\tvalidation_1-rmse:0.902481\n",
      "[58]\tvalidation_0-rmse:0.810054\tvalidation_1-rmse:0.902312\n",
      "[59]\tvalidation_0-rmse:0.809703\tvalidation_1-rmse:0.901965\n",
      "[60]\tvalidation_0-rmse:0.809235\tvalidation_1-rmse:0.901833\n",
      "[61]\tvalidation_0-rmse:0.808624\tvalidation_1-rmse:0.901946\n",
      "[62]\tvalidation_0-rmse:0.808406\tvalidation_1-rmse:0.90208\n",
      "[63]\tvalidation_0-rmse:0.808139\tvalidation_1-rmse:0.902069\n",
      "[64]\tvalidation_0-rmse:0.807753\tvalidation_1-rmse:0.902386\n",
      "[65]\tvalidation_0-rmse:0.807427\tvalidation_1-rmse:0.902438\n",
      "[66]\tvalidation_0-rmse:0.807295\tvalidation_1-rmse:0.902233\n",
      "[67]\tvalidation_0-rmse:0.807039\tvalidation_1-rmse:0.90208\n",
      "[68]\tvalidation_0-rmse:0.806669\tvalidation_1-rmse:0.901779\n",
      "[69]\tvalidation_0-rmse:0.806437\tvalidation_1-rmse:0.901944\n",
      "[70]\tvalidation_0-rmse:0.806053\tvalidation_1-rmse:0.902484\n",
      "[71]\tvalidation_0-rmse:0.805746\tvalidation_1-rmse:0.902355\n",
      "[72]\tvalidation_0-rmse:0.805471\tvalidation_1-rmse:0.902289\n",
      "[73]\tvalidation_0-rmse:0.805342\tvalidation_1-rmse:0.902329\n",
      "[74]\tvalidation_0-rmse:0.804962\tvalidation_1-rmse:0.902324\n",
      "[75]\tvalidation_0-rmse:0.8044\tvalidation_1-rmse:0.901626\n",
      "[76]\tvalidation_0-rmse:0.804035\tvalidation_1-rmse:0.901402\n",
      "[77]\tvalidation_0-rmse:0.803706\tvalidation_1-rmse:0.901534\n",
      "[78]\tvalidation_0-rmse:0.80352\tvalidation_1-rmse:0.901616\n",
      "[79]\tvalidation_0-rmse:0.803241\tvalidation_1-rmse:0.901483\n",
      "[80]\tvalidation_0-rmse:0.802935\tvalidation_1-rmse:0.901491\n",
      "[81]\tvalidation_0-rmse:0.802698\tvalidation_1-rmse:0.901373\n",
      "[82]\tvalidation_0-rmse:0.802519\tvalidation_1-rmse:0.901285\n",
      "[83]\tvalidation_0-rmse:0.802253\tvalidation_1-rmse:0.901489\n",
      "[84]\tvalidation_0-rmse:0.801568\tvalidation_1-rmse:0.901292\n",
      "[85]\tvalidation_0-rmse:0.801274\tvalidation_1-rmse:0.900884\n",
      "[86]\tvalidation_0-rmse:0.801087\tvalidation_1-rmse:0.900767\n",
      "[87]\tvalidation_0-rmse:0.800729\tvalidation_1-rmse:0.900799\n",
      "[88]\tvalidation_0-rmse:0.80023\tvalidation_1-rmse:0.900716\n",
      "[89]\tvalidation_0-rmse:0.799812\tvalidation_1-rmse:0.900866\n",
      "[90]\tvalidation_0-rmse:0.799509\tvalidation_1-rmse:0.900839\n",
      "[91]\tvalidation_0-rmse:0.799352\tvalidation_1-rmse:0.900974\n",
      "[92]\tvalidation_0-rmse:0.799162\tvalidation_1-rmse:0.901181\n",
      "[93]\tvalidation_0-rmse:0.798998\tvalidation_1-rmse:0.901256\n",
      "[94]\tvalidation_0-rmse:0.798463\tvalidation_1-rmse:0.901158\n",
      "[95]\tvalidation_0-rmse:0.798206\tvalidation_1-rmse:0.901015\n",
      "[96]\tvalidation_0-rmse:0.798032\tvalidation_1-rmse:0.901197\n",
      "[97]\tvalidation_0-rmse:0.797842\tvalidation_1-rmse:0.901195\n",
      "[98]\tvalidation_0-rmse:0.797553\tvalidation_1-rmse:0.901148\n",
      "Stopping. Best iteration:\n",
      "[88]\tvalidation_0-rmse:0.80023\tvalidation_1-rmse:0.900716\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8549.392746925354"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "model = XGBRegressor(\n",
    "    max_depth=8,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3,    \n",
    "    seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 310\n"
     ]
    }
   ],
   "source": [
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid).clip(0, 20)\n",
    "Y_test = model.predict(X_test).clip(0, 20)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_data.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('xgb_submission.csv', index=False)\n",
    "\n",
    "# save predictions for an ensemble\n",
    "pickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\n",
    "pickle.dump(Y_test, open('xgb_test.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model/XGB.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce3a3c4518>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_features(model, (10,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "projectdetails": {
   "creator_id": "vgupt137",
   "notebook_id": "377d4d6e-fb18-40b0-b880-7acfac683bfe",
   "notebook_name": "EDA and feature creation .ipynb",
   "prod_sys": null,
   "project_desc": "",
   "project_id": "5ac56d77-4579-490b-a6ec-d228a507a97a",
   "project_name": "coursera",
   "project_status": null,
   "status": "new"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
